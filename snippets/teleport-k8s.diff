diff --git a/go.mod b/go.mod
index 534f054e56..57ddd65570 100644
--- a/go.mod
+++ b/go.mod
@@ -264,6 +264,11 @@ require (
 	software.sslmate.com/src/go-pkcs12 v0.5.0
 )
 
+require (
+	github.com/goccy/go-json v0.10.5
+	github.com/joho/godotenv v1.5.1
+)
+
 require (
 	cel.dev/expr v0.23.1 // indirect
 	cloud.google.com/go v0.121.1 // indirect
@@ -385,7 +390,6 @@ require (
 	github.com/gobwas/glob v0.2.3 // indirect
 	github.com/gobwas/httphead v0.1.0 // indirect
 	github.com/gobwas/pool v0.2.1 // indirect
-	github.com/goccy/go-json v0.10.5 // indirect
 	github.com/godbus/dbus v0.0.0-20190726142602-4481cbc300e2 // indirect
 	github.com/godbus/dbus/v5 v5.1.0 // indirect
 	github.com/golang-sql/civil v0.0.0-20220223132316-b832511892a9 // indirect
@@ -612,3 +616,5 @@ replace (
 // when evaluating versions; "go get -u ./..." succeeding is a good sign that
 // the problem has been resolved
 exclude google.golang.org/grpc/stats/opentelemetry v0.0.0-20241028142157-ada6787961b3
+
+replace github.com/gravitational/teleport => ./
diff --git a/go.sum b/go.sum
index 3244e104ba..0d7932a970 100644
--- a/go.sum
+++ b/go.sum
@@ -1732,6 +1732,8 @@ github.com/jmoiron/sqlx v1.4.0 h1:1PLqN7S1UYp5t4SrVVnt4nUVNemrDAtxlulVe+Qgm3o=
 github.com/jmoiron/sqlx v1.4.0/go.mod h1:ZrZ7UsYB/weZdl2Bxg6jCRO9c3YHl8r3ahlKmRT4JLY=
 github.com/johannesboyne/gofakes3 v0.0.0-20240217095638-c55a48f17be6 h1:W8heH5NR7dfdB4FehSFI+DxjCbVKe9fPkPqKzCPJwnM=
 github.com/johannesboyne/gofakes3 v0.0.0-20240217095638-c55a48f17be6/go.mod h1:AxgWC4DDX54O2WDoQO1Ceabtn6IbktjU/7bigor+66g=
+github.com/joho/godotenv v1.5.1 h1:7eLL/+HRGLY0ldzfGMeQkb7vMd0as4CfYvUVzLqw0N0=
+github.com/joho/godotenv v1.5.1/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=
 github.com/jonboulle/clockwork v0.5.0 h1:Hyh9A8u51kptdkR+cqRpT1EebBwTn1oK9YfGYbdFz6I=
 github.com/jonboulle/clockwork v0.5.0/go.mod h1:3mZlmanh0g2NDKO5TWZVJAfofYk64M7XN3SzBPjZF60=
 github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
diff --git a/lib/client/trusted_certs_store.go b/lib/client/trusted_certs_store.go
index 24dede0c3a..6213e99902 100644
--- a/lib/client/trusted_certs_store.go
+++ b/lib/client/trusted_certs_store.go
@@ -392,6 +392,7 @@ func (fs *FSTrustedCertsStore) addKnownHosts(proxyHost string, cas []authclient.
 	// add every host key to the list of entries
 	for _, ca := range cas {
 		for _, hostKey := range ca.AuthorizedKeys {
+			fs.log.DebugContext(context.Background(), "üêõ Starting to store host...")
 			fs.log.DebugContext(context.Background(), "Adding known host entry",
 				"cluster_name", ca.ClusterName,
 				"proxy", proxyHost,
diff --git a/lib/kube/dev-pod-reader-binding.yaml b/lib/kube/dev-pod-reader-binding.yaml
new file mode 100644
index 0000000000..60c874b333
--- /dev/null
+++ b/lib/kube/dev-pod-reader-binding.yaml
@@ -0,0 +1,34 @@
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: dev-pod-reader-binding
+  namespace: dev
+subjects:
+  - kind: Group
+    name: dev-pod-reader   # Teleport RoleÏùò kubernetes_groupsÏôÄ ÏùºÏπò
+    apiGroup: rbac.authorization.k8s.io
+  - kind: User
+    name: testuser         # Ï†ïÌôïÌïú ÏÇ¨Ïö©ÏûêÎ™Ö (kubectl config view ÏóêÏÑú ÌôïÏù∏)
+    apiGroup: rbac.authorization.k8s.io
+roleRef:
+  kind: Role
+  name: dev-pod-reader
+  apiGroup: rbac.authorization.k8s.io
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: RoleBinding
+metadata:
+  name: dev-pod-reader-binding
+  namespace: kube-system
+subjects:
+  - kind: Group
+    name: dev-pod-reader
+    apiGroup: rbac.authorization.k8s.io
+  - kind: User
+    name: testuser
+    apiGroup: rbac.authorization.k8s.io
+roleRef:
+  kind: Role
+  name: dev-pod-reader
+  apiGroup: rbac.authorization.k8s.io
diff --git a/lib/kube/dev-pod-reader-roles.yaml b/lib/kube/dev-pod-reader-roles.yaml
new file mode 100644
index 0000000000..0b81fdcbd2
--- /dev/null
+++ b/lib/kube/dev-pod-reader-roles.yaml
@@ -0,0 +1,20 @@
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: dev-pod-reader
+  namespace: dev
+rules:
+  - apiGroups: [""]
+    resources: ["pods"]
+    verbs: ["get", "list", "watch"]
+
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: Role
+metadata:
+  name: dev-pod-reader
+  namespace: kube-system
+rules:
+  - apiGroups: [""]
+    resources: ["pods"]
+    verbs: ["get", "list", "watch"]
diff --git a/lib/kube/dev-pod-reader.yaml b/lib/kube/dev-pod-reader.yaml
new file mode 100644
index 0000000000..e4ba30209b
--- /dev/null
+++ b/lib/kube/dev-pod-reader.yaml
@@ -0,0 +1,41 @@
+kind: role
+metadata:
+  name: dev-pod-reader
+version: v8
+spec:
+  allow:
+    kubernetes_labels:
+      '*': '*'
+    kubernetes_resources:
+      # Pods
+      - kind: pods
+        namespace: dev
+        name: "*"
+        verbs: ["*"]
+        api_group: ""
+      - kind: pods
+        namespace: kube-system
+        name: "*"
+        verbs: ["*"]
+        api_group: ""
+      # Roles
+      - kind: roles
+        namespace: dev
+        name: "*"
+        verbs: ["*"]
+        api_group: rbac.authorization.k8s.io
+      # RoleBindings
+      - kind: rolebindings
+        namespace: dev
+        name: "*"
+        verbs: ["*"]
+        api_group: rbac.authorization.k8s.io
+    kubernetes_groups:
+      - dev-pod-reader
+  deny:
+    kubernetes_resources:
+      - kind: "*"
+        namespace: "ops"
+        name: "*"
+        verbs: ["*"]
+        api_group: "*"   # Ïó¨Í∏∞ Ï∂îÍ∞Ä
diff --git a/lib/kube/watcher/match_go_role_test.go b/lib/kube/watcher/match_go_role_test.go
new file mode 100644
index 0000000000..a98f929f0a
--- /dev/null
+++ b/lib/kube/watcher/match_go_role_test.go
@@ -0,0 +1,53 @@
+package watcher
+
+import (
+	"context"
+	"os"
+	"testing"
+
+	"github.com/gravitational/teleport/api/types"
+	"github.com/gravitational/teleport/lib/services"
+	"github.com/joho/godotenv"
+	"github.com/stretchr/testify/require"
+)
+
+func TestMatchPodAccessAndNotify(t *testing.T) {
+	err := godotenv.Load(".env")
+	require.NoError(t, err, "failed to load .env.test")
+
+	webhookURL := os.Getenv("WEBHOOK_URI")
+	require.NotEmpty(t, webhookURL, "WEBHOOK_URI must be set in .env.test")
+
+	ctx := context.Background()
+
+	cluster, err := types.NewKubernetesClusterV3(
+		types.Metadata{
+			Name:   "test-cluster",
+			Labels: map[string]string{"env": "dev"},
+		},
+		types.KubernetesClusterSpecV3{},
+	)
+	require.NoError(t, err)
+
+	pod := types.KubernetesResource{
+		Kind:      "pods",
+		Name:      "nginx",
+		Namespace: "default",
+	}
+
+	denyRole, err := types.NewRole("deny-role", types.RoleSpecV6{
+		Deny: types.RoleConditions{
+			KubernetesLabels: types.Labels{"env": []string{"dev"}},
+			KubernetesResources: []types.KubernetesResource{
+				pod,
+			},
+		},
+	})
+	require.NoError(t, err)
+
+	roleSet := services.RoleSet{denyRole}
+	userTraits := map[string][]string{}
+
+	err = MatchPodAccessAndNotify(ctx, pod, cluster, roleSet, userTraits, webhookURL)
+	require.NoError(t, err)
+}
diff --git a/lib/kube/watcher/match_pod_role.go b/lib/kube/watcher/match_pod_role.go
new file mode 100644
index 0000000000..5039c97a1f
--- /dev/null
+++ b/lib/kube/watcher/match_pod_role.go
@@ -0,0 +1,125 @@
+package watcher
+
+import (
+	"bytes"
+	"context"
+	"fmt"
+	"io"
+	"log"
+	"net/http"
+
+	"github.com/goccy/go-json"
+	"github.com/gravitational/teleport/api/types"
+	"github.com/gravitational/teleport/lib/services"
+	"github.com/gravitational/trace"
+)
+
+// MatchPodAccessAndNotify checks if the user is allowed to access the specified pod.
+// If access is denied, it logs the denial and optionally sends an alert (e.g., Slack, email, webhook).
+// If access is granted, it logs the successful access.
+func MatchPodAccessAndNotify(
+	ctx context.Context,
+	podResource PodResourceWithLabels,
+	roleSet services.RoleSet,
+	userTraits map[string][]string,
+	clusterName string,
+	webhookURL string) error {
+	//podResource := types.KubernetesResource{
+	//	Kind:      "pods",
+	//	Namespace: pod.Namespace,
+	//	Name:      pod.Name,
+	//	Verbs:     []string{"get"},
+	//}
+	//
+	//cluster, err := types.NewKubernetesClusterV3(types.Metadata{
+	//	Name:   clusterName,
+	//	Labels: map[string]string{"env": "dev"}, // or actual label from metadata
+	//}, types.KubernetesClusterSpecV3{})
+	//if err != nil {
+	//	log.Error(err)
+	//	return
+	//}
+
+	cluster, err := types.NewKubernetesClusterV3(types.Metadata{
+		Name:   clusterName,
+		Labels: map[string]string{}, // Îπà mapÏúºÎ°ú Îë†, ÌïÑÏöî Ïãú Ïã§Ï†ú Î†àÏù¥Î∏î ÎÑ£Í∏∞
+	}, types.KubernetesClusterSpecV3{})
+	if err != nil {
+		log.Printf("Failed to create cluster object: %v", err)
+		return trace.Wrap(err)
+	}
+
+	kubernetesResource := types.KubernetesResource{
+		Kind:      podResource.Kind,
+		Namespace: podResource.Namespace,
+		Name:      podResource.Name,
+		Verbs:     []string{"get", "list"}, // Default verbs for pod access check
+	}
+
+	// Create a dummy cluster for access check
+	// TODO: Get actual cluster information from current session
+	/*cluster, err := types.NewKubernetesClusterV3(types.Metadata{
+		Name:   "default-cluster",                      // This should come from user session
+		Labels: map[string]string{"env": "production"}, // This should come from actual cluster metadata
+	}, types.KubernetesClusterSpecV3{})
+	if err != nil {
+		log.Printf("Failed to create cluster object: %v", err)
+		return trace.Wrap(err)
+	}*/
+
+	log.Printf("Checking access for pod: %s/%s with labels: %v, clusterName: %s",
+		podResource.Namespace, podResource.Name, podResource.Labels, cluster.GetName())
+
+	err = roleSet.CheckAccessToPod(ctx, cluster, kubernetesResource, userTraits)
+	// CheckAccessToPod checks if the user has access to the specified pod.
+	if err != nil {
+		if trace.IsAccessDenied(err) {
+			// Access denied ‚Äî log the event
+			log.Printf("‚ùå Access DENIED to pod %s/%s: %v", podResource.Namespace, podResource.Name, err)
+
+			if webhookURL != "" {
+				msg := fmt.Sprintf("Access DENIED to pod: %s in namespace: %s", podResource.Name, podResource.Namespace)
+				if err := sendWebhookAlert(webhookURL, msg); err != nil {
+					log.Printf("‚ùóÔ∏è Failed to send webhook alert: %v", err)
+				}
+			}
+
+			return nil // Denial is a handled, expected outcome
+		}
+		// nexpected error ‚Äî return wrapped error
+		return trace.Wrap(err)
+	}
+
+	// Access granted ‚Äî log the event
+	log.Printf("‚≠êÔ∏è Access ALLOWED to pod %q", podResource.Name)
+	return nil
+}
+
+// sendWebhookAlert sends a POST request to the given webhook URL with the alert message
+func sendWebhookAlert(webhookURL string, message string) error {
+	payload := map[string]string{"text": message}
+	jsonPayload, err := json.Marshal(payload)
+	if err != nil {
+		return err
+	}
+
+	req, err := http.NewRequest("POST", webhookURL, bytes.NewBuffer(jsonPayload))
+	if err != nil {
+		return err
+	}
+	req.Header.Set("Content-Type", "application/json")
+
+	client := &http.Client{}
+	resp, err := client.Do(req)
+	if err != nil {
+		return err
+	}
+	defer resp.Body.Close()
+
+	if resp.StatusCode != http.StatusOK {
+		body, _ := io.ReadAll(resp.Body)
+		return fmt.Errorf("webhook failed: %s", string(body))
+	}
+
+	return nil
+}
diff --git a/lib/kube/watcher/notifier.go b/lib/kube/watcher/notifier.go
new file mode 100644
index 0000000000..fe45cff669
--- /dev/null
+++ b/lib/kube/watcher/notifier.go
@@ -0,0 +1,41 @@
+package watcher
+
+import (
+	"bytes"
+	"encoding/json"
+	"fmt"
+	"net/http"
+	"os"
+)
+
+type slackPayload struct {
+	Text string `json:"text"`
+}
+
+func sendSlackNotification(webhookURL, message string) error {
+	webhookURL = os.Getenv("SLACK_WEBHOOK_URL")
+	if webhookURL == "" {
+		return nil
+	}
+
+	payload := slackPayload{
+		Text: message,
+	}
+
+	data, err := json.Marshal(payload)
+	if err != nil {
+		return fmt.Errorf("failed to marshal payload: %w", err)
+	}
+
+	resp, err := http.Post(webhookURL, "application/json", bytes.NewBuffer(data))
+	if err != nil {
+		return fmt.Errorf("failed to send POST request: %w", err)
+	}
+	defer resp.Body.Close()
+
+	if resp.StatusCode != http.StatusOK {
+		return fmt.Errorf("Slack responded with non-OK status: %s", resp.Status)
+	}
+
+	return nil
+}
diff --git a/lib/kube/watcher/watcher.go b/lib/kube/watcher/watcher.go
new file mode 100644
index 0000000000..1595d07cef
--- /dev/null
+++ b/lib/kube/watcher/watcher.go
@@ -0,0 +1,188 @@
+package watcher
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"log"
+	"sync"
+	"time"
+
+	"github.com/gravitational/teleport/api/types"
+	"github.com/gravitational/teleport/lib/services"
+	corev1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/rest"
+	"k8s.io/client-go/tools/clientcmd"
+	"k8s.io/client-go/util/homedir"
+)
+
+type PodResourceWithLabels struct {
+	types.KubernetesResource
+	Labels map[string]string `json:"labels,omitempty"`
+}
+
+type UserSession struct {
+	Username   string
+	RoleSet    services.RoleSet
+	UserTraits map[string][]string
+	Cluster    string
+	ProxyURL   string
+	LoginTime  time.Time
+}
+
+var (
+	currentUserSession *UserSession
+	sessionMutex       sync.RWMutex
+)
+
+var (
+	cancelFunc context.CancelFunc
+	cancelOnce sync.Once
+)
+
+func UpdateUserSession(session *UserSession) {
+	sessionMutex.Lock()
+	defer sessionMutex.Unlock()
+	currentUserSession = session
+	log.Printf("üíª [UpdateUserSession] Session set for user: %s", session.Username)
+	log.Printf("üíª [UpdateUserSession] Session set for user: %s", session.Cluster)
+	log.Printf("üíª [UpdateUserSession] Session set for user: %s", session.RoleSet)
+	log.Printf("üíª [UpdateUserSession] Session set for user: %s", session.UserTraits)
+}
+
+func GetCurrentUserSession() *UserSession {
+	sessionMutex.RLock()
+	defer sessionMutex.RUnlock()
+	if currentUserSession == nil {
+		log.Println("üíª [GetCurrentUserSession] currentUserSession is nil")
+	} else {
+		log.Printf("üíª [GetCurrentUserSession] currentUserSession for user: %s", currentUserSession.Username)
+	}
+	return currentUserSession
+}
+
+func StringsToRoleSet(roles []string) services.RoleSet {
+	roleSet := make(services.RoleSet, 0, len(roles))
+	for _, roleName := range roles {
+		roleSet = append(roleSet, &types.RoleV6{
+			Metadata: types.Metadata{
+				Name: roleName,
+			},
+		})
+	}
+	return roleSet
+}
+
+func StartPodWatcher(ctx context.Context, webhookURL string) error {
+
+	log.Println("üöÄ [StartPodWatcher] Pod watcher started")
+
+	session := GetCurrentUserSession()
+	if session == nil {
+		log.Println("üö´ [StartPodWatcher] No user session available")
+		return nil
+	}
+
+	log.Printf("‚úÖ [StartPodWatcher] Found user session for: %s", session.Username)
+
+	var kubeconfig string
+	if home := homedir.HomeDir(); home != "" {
+		kubeconfig = home + "/.kube/config"
+	}
+
+	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
+	if err != nil {
+		config, err = rest.InClusterConfig()
+		if err != nil {
+			return fmt.Errorf("failed to get in-cluster config: %w", err)
+		}
+	}
+
+	clientset, err := kubernetes.NewForConfig(config)
+	if err != nil {
+		return fmt.Errorf("failed to create k8s client: %w", err)
+	}
+
+	watcher, err := clientset.CoreV1().Pods("").Watch(ctx, metav1.ListOptions{})
+	if err != nil {
+		return fmt.Errorf("failed to pod watcher: %w", err)
+	}
+	defer watcher.Stop()
+
+	for event := range watcher.ResultChan() {
+		pod, ok := event.Object.(*corev1.Pod)
+		if !ok {
+			continue
+		}
+
+		eventType := event.Type
+		podName := pod.GetName()
+		namespace := pod.GetNamespace()
+		labels := pod.GetLabels()
+
+		/*kubernetesResource := types.KubernetesResource{
+			Kind:      "pods",
+			Name:      podName,
+			Namespace: namespace,
+		}
+
+		PodResourceWithLabels := PodResourceWithLabels{
+			KubernetesResource: kubernetesResource,
+			Labels:             labels,
+		}*/
+
+		PodResourceWithLabels := PodResourceWithLabels{
+			KubernetesResource: types.KubernetesResource{
+				Kind:      "pods",
+				Name:      podName,
+				Namespace: namespace,
+			},
+			Labels: labels,
+		}
+
+		userSession := GetCurrentUserSession()
+		if userSession != nil {
+			log.Printf("Checking pod access for user: %s", userSession.Username)
+			err := MatchPodAccessAndNotify(
+				ctx,
+				PodResourceWithLabels,
+				userSession.RoleSet,
+				userSession.UserTraits,
+				userSession.Cluster,
+				webhookURL,
+			)
+			if err != nil {
+				log.Printf("Failed to match pod access: %v", err)
+			}
+		} else {
+			log.Println("üö® No user session available, skipping access check")
+		}
+
+		resourceJSON, err := json.MarshalIndent(PodResourceWithLabels, "", " ")
+		if err != nil {
+			log.Printf("Failed to marshal PodResourceWithLabels: %v", err)
+			continue
+		}
+
+		labelStr := ""
+		for k, v := range labels {
+			labelStr += fmt.Sprintf("%s=%s ", k, v)
+		}
+
+		message := fmt.Sprintf(
+			"*[%s]* PodResourceWithLabels:\n```json\n%s\n```\nTime: %s",
+			eventType, string(resourceJSON), time.Now().Format(time.RFC3339),
+		)
+
+		err = sendSlackNotification(webhookURL, message)
+		if err != nil {
+			log.Printf("Failed to send Slack notification: %v", err)
+		}
+
+		log.Printf("Pod event: %s - %s/%s with labels: %v", eventType, namespace, podName, labels)
+	}
+
+	return nil
+}
diff --git a/lib/services/role.go b/lib/services/role.go
index 4251f2cbae..fbb1806e96 100644
--- a/lib/services/role.go
+++ b/lib/services/role.go
@@ -2581,6 +2581,43 @@ func (l kubernetesClusterLabelMatcher) getKubeLabelMatchers(role types.Role, typ
 	return labelMatchers, nil
 }
 
+// CheckAccessToPod checks if the user has access to the specified Kubernetes Pod
+// in a given Kubernetes cluster, based on their RoleSet and the cluster's label constraints.
+// It first checks deny rules, then allow rules, and returns an error if access is denied.
+func (set RoleSet) CheckAccessToPod(ctx context.Context, cluster types.KubeCluster, pod types.KubernetesResource, userTraits wrappers.Traits) error {
+
+	//check deny: a single match on a deny rule prohibits access
+	for _, role := range set {
+		podMatcher := NewKubeResourcesMatcher([]types.KubernetesResource{pod})
+		matched, err := podMatcher.Match(role, types.Deny)
+		if err != nil {
+			return trace.Wrap(err)
+		}
+		if matched {
+			return trace.AccessDenied("access to pod %q denied by role %q", pod.Name, role.GetName())
+		}
+	}
+
+	// check allow: if matches, allow access
+	for _, role := range set {
+		matchLabels, _, err := checkRoleLabelsMatch(types.Allow, role, userTraits, cluster, false)
+		if err != nil || !matchLabels {
+			continue
+		}
+
+		podMatcher := NewKubeResourcesMatcher([]types.KubernetesResource{pod})
+		matched, err := podMatcher.Match(role, types.Allow)
+		if err != nil {
+			return trace.Wrap(err)
+		}
+		if matched {
+			return nil // access allowed
+		} // If matched, access is allowed.
+	}
+
+	return trace.AccessDenied("no role allows access to pod %q", pod.Name)
+}
+
 // AccessCheckable is the subset of types.Resource required for the RBAC checks.
 type AccessCheckable interface {
 	GetKind() string
diff --git a/lib/services/role_test.go b/lib/services/role_test.go
index 7357530588..e7362149e6 100644
--- a/lib/services/role_test.go
+++ b/lib/services/role_test.go
@@ -9529,6 +9529,120 @@ func TestKubeResourcesMatcher(t *testing.T) {
 		})
 	}
 }
+func TestCheckAccessToPod(t *testing.T) {
+	ctx := context.Background()
+
+	basePod := types.KubernetesResource{
+		Kind:      "pods",
+		Name:      "nginx",
+		Namespace: "default",
+	}
+
+	cluster, err := types.NewKubernetesClusterV3(
+		types.Metadata{
+			Name:   "test-cluster",
+			Labels: map[string]string{"env": "dev"},
+		},
+		types.KubernetesClusterSpecV3{},
+	)
+	require.NoError(t, err)
+
+	userTraits := wrappers.Traits{}
+
+	t.Run("AllowExactMatch", func(t *testing.T) {
+		role, _ := types.NewRole("allow-role", types.RoleSpecV6{
+			Allow: types.RoleConditions{
+				KubernetesLabels: types.Labels{"env": []string{"dev"}},
+				KubernetesResources: []types.KubernetesResource{
+					basePod,
+				},
+			},
+		})
+		err := RoleSet{role}.CheckAccessToPod(ctx, cluster, basePod, userTraits)
+		require.NoError(t, err)
+	})
+
+	t.Run("DenyExactMatch", func(t *testing.T) {
+		role, _ := types.NewRole("deny-role", types.RoleSpecV6{
+			Deny: types.RoleConditions{
+				KubernetesLabels: types.Labels{"env": []string{"dev"}},
+				KubernetesResources: []types.KubernetesResource{
+					basePod,
+				},
+			},
+		})
+		err := RoleSet{role}.CheckAccessToPod(ctx, cluster, basePod, userTraits)
+		require.Error(t, err)
+		require.True(t, trace.IsAccessDenied(err))
+	})
+
+	t.Run("AllowButLabelMismatch", func(t *testing.T) {
+		role, _ := types.NewRole("label-mismatch", types.RoleSpecV6{
+			Allow: types.RoleConditions{
+				KubernetesLabels: types.Labels{"env": []string{"prod"}},
+				KubernetesResources: []types.KubernetesResource{
+					basePod,
+				},
+			},
+		})
+		err := RoleSet{role}.CheckAccessToPod(ctx, cluster, basePod, userTraits)
+		require.Error(t, err)
+		require.True(t, trace.IsAccessDenied(err))
+	})
+
+	t.Run("AllowWithWildcardName", func(t *testing.T) {
+		role, _ := types.NewRole("wildcard-name", types.RoleSpecV6{
+			Allow: types.RoleConditions{
+				KubernetesLabels: types.Labels{"env": []string{"dev"}},
+				KubernetesResources: []types.KubernetesResource{
+					{
+						Kind:      "pods",
+						Name:      "*",
+						Namespace: "default",
+					},
+				},
+			},
+		})
+		err := RoleSet{role}.CheckAccessToPod(ctx, cluster, basePod, userTraits)
+		require.NoError(t, err)
+	})
+
+	t.Run("NamespaceMismatch", func(t *testing.T) {
+		role, _ := types.NewRole("ns-mismatch", types.RoleSpecV6{
+			Allow: types.RoleConditions{
+				KubernetesLabels: types.Labels{"env": []string{"dev"}},
+				KubernetesResources: []types.KubernetesResource{
+					{
+						Kind:      "pods",
+						Name:      "nginx",
+						Namespace: "prod",
+					},
+				},
+			},
+		})
+		err := RoleSet{role}.CheckAccessToPod(ctx, cluster, basePod, userTraits)
+		require.Error(t, err)
+		require.True(t, trace.IsAccessDenied(err))
+	})
+
+	t.Run("NoMatchAtAll", func(t *testing.T) {
+		role, _ := types.NewRole("irrelevant-role", types.RoleSpecV6{
+			Allow: types.RoleConditions{
+				KubernetesLabels: types.Labels{"env": []string{"dev"}},
+				KubernetesResources: []types.KubernetesResource{
+					{
+						Kind:      "pods",
+						Name:      "other-pod",
+						Namespace: "default",
+					},
+				},
+			},
+		})
+		err := RoleSet{role}.CheckAccessToPod(ctx, cluster, basePod, userTraits)
+		require.Error(t, err)
+		require.True(t, trace.IsAccessDenied(err))
+	})
+}
 
 func boolsToSlice(v ...bool) []bool {
 	return v
diff --git a/lib/web/apiserver.go b/lib/web/apiserver.go
index 4a7486785f..bc4efd277a 100644
--- a/lib/web/apiserver.go
+++ b/lib/web/apiserver.go
@@ -30,6 +30,7 @@ import (
 	"fmt"
 	"html/template"
 	"io"
+	"log"
 	"log/slog"
 	"math/rand/v2"
 	"net"
@@ -89,6 +90,7 @@ import (
 	"github.com/gravitational/teleport/lib/httplib"
 	"github.com/gravitational/teleport/lib/httplib/csrf"
 	"github.com/gravitational/teleport/lib/jwt"
+	"github.com/gravitational/teleport/lib/kube/watcher"
 	"github.com/gravitational/teleport/lib/limiter"
 	"github.com/gravitational/teleport/lib/modules"
 	"github.com/gravitational/teleport/lib/multiplexer"
@@ -2465,6 +2467,7 @@ func newSessionResponse(sctx *SessionContext) (*CreateSessionResponse, error) {
 //
 // {"type": "bearer", "token": "bearer token", "user": {"name": "alex", "allowed_logins": ["admin", "bob"]}, "expires_in": 20}
 func (h *Handler) createWebSession(w http.ResponseWriter, r *http.Request, p httprouter.Params) (any, error) {
+	log.Println("üêõ [createWebSession] called")
 	var req *CreateSessionReq
 	if err := httplib.ReadResourceJSON(r, &req); err != nil {
 		return nil, trace.Wrap(err)
@@ -2512,7 +2515,42 @@ func (h *Handler) createWebSession(w http.ResponseWriter, r *http.Request, p htt
 		return nil, trace.AccessDenied("need auth")
 	}
 
+	accessChecker, err := ctx.GetUserAccessChecker()
+	if err != nil {
+		h.logger.WarnContext(r.Context(), "Failed to get access checker", "user", req.User, "error", err)
+		return nil, trace.Wrap(err)
+	}
+
+	clt, err := ctx.GetClient()
+	if err != nil {
+		return nil, trace.Wrap(err)
+	}
+	clusterName, err := clt.GetClusterName(r.Context())
+	if err != nil {
+		return nil, trace.Wrap(err)
+	}
+
+	clusterNameStr := clusterName.GetClusterName()
+
+	watcher.UpdateUserSession(&watcher.UserSession{
+		Username:   req.User,
+		Cluster:    clusterNameStr,
+		RoleSet:    accessChecker.Roles(),
+		UserTraits: accessChecker.Traits(),
+	})
+
+	webhookURL := os.Getenv("SLACK_WEBHOOK_URL")
+
+	go func() {
+		err := watcher.StartPodWatcher(context.Background(), webhookURL)
+		if err != nil {
+			log.Printf("error: %v", err)
+		}
+	}()
+
+	log.Println("üëΩ Before newSessionResponse")
 	res, err := newSessionResponse(ctx)
+	log.Println("üëΩ After newSessionResponse, returning response")
 	return res, trace.Wrap(err)
 }
 
diff --git a/lib/web/app/handler.go b/lib/web/app/handler.go
index 2429fcc27e..acc3e87332 100644
--- a/lib/web/app/handler.go
+++ b/lib/web/app/handler.go
@@ -25,6 +25,7 @@ import (
 	"crypto/tls"
 	"fmt"
 	"io"
+	"log"
 	"log/slog"
 	"net"
 	"net/http"
@@ -145,11 +146,25 @@ func NewHandler(ctx context.Context, c *HandlerConfig) (*Handler, error) {
 	h.router.GET("/x-teleport-auth", makeRouterHandler(h.startAppAuthExchange))
 	h.router.POST("/x-teleport-auth", makeRouterHandler(h.completeAppAuthExchange))
 	h.router.GET("/teleport-logout", h.withRouterAuth(h.handleLogout))
+	h.router.POST("/v1/testapi/sessions/web", makeRouterHandler(h.createWebSessionHandler))
 	h.router.NotFound = h.withAuth(h.handleHttp)
 
 	return h, nil
 }
 
+func (h *Handler) createWebSessionHandler(w http.ResponseWriter, r *http.Request, p httprouter.Params) error {
+	log.Println("üêõ [Custom Web Session Handler] called")
+
+	w.Header().Set("Content-Type", "application/json")
+	w.WriteHeader(http.StatusOK)
+	_, err := w.Write([]byte(`{"message":"‚úÖ Custom web session created"}`))
+	if err != nil {
+		log.Println("write error:", err)
+		return trace.Wrap(err)
+	}
+	return nil
+}
+
 // ServeHTTP hands the request to the request router.
 func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
 	h.router.ServeHTTP(w, r)
diff --git a/lib/web/app/middleware.go b/lib/web/app/middleware.go
index 2d364e8bc1..64c5221126 100644
--- a/lib/web/app/middleware.go
+++ b/lib/web/app/middleware.go
@@ -19,6 +19,7 @@
 package app
 
 import (
+	"log"
 	"net/http"
 
 	"github.com/gravitational/trace"
@@ -102,6 +103,7 @@ func (h *Handler) redirectToLauncher(w http.ResponseWriter, r *http.Request, p l
 func makeRouterHandler(handler routerFunc) httprouter.Handle {
 	return func(w http.ResponseWriter, r *http.Request, p httprouter.Params) {
 		if err := handler(w, r, p); err != nil {
+			log.Printf("[makeRouterHandler] handler error: %v\n", err)
 			writeError(w, err)
 			return
 		}
diff --git a/tool/teleport/common/teleport.go b/tool/teleport/common/teleport.go
index 6897e94b3a..4a4fbc5445 100644
--- a/tool/teleport/common/teleport.go
+++ b/tool/teleport/common/teleport.go
@@ -816,6 +816,22 @@ func OnStart(clf config.CommandLineFlags, config *servicecfg.Config) error {
 	} else {
 		config.Logger.InfoContext(ctx, "Starting Teleport with a config file", "version", teleport.Version, "config_file", configFileUsed)
 	}
+
+	/*webhookURL := os.Getenv("SLACK_WEBHOOK_URL")
+	config.Logger.InfoContext(ctx, "DEBUG: SLACK_WEBHOOK_URL value", "url", webhookURL, "length", len(webhookURL))
+
+	if webhookURL != "" {
+		config.Logger.InfoContext(ctx, "Starting Pod Watcher with Slack notifications")
+		go func() {
+			time.Sleep(15 * time.Second)
+			if err := watcher.StartPodWatcher(ctx, webhookURL); err != nil {
+				config.Logger.ErrorContext(ctx, "Pod Watcher error", "error", err)
+			}
+		}()
+	} else {
+		config.Logger.InfoContext(ctx, "Pod watcher disabled - no SLACK_WEBHOOK_URL provided")
+	} */
+
 	return service.Run(ctx, *config, nil)
 }
 
diff --git a/tool/tsh/common/tsh.go b/tool/tsh/common/tsh.go
index 0d2ca27c1f..4dd13e37d5 100644
--- a/tool/tsh/common/tsh.go
+++ b/tool/tsh/common/tsh.go
@@ -50,6 +50,7 @@ import (
 	"github.com/google/uuid"
 	"github.com/gravitational/trace"
 	"github.com/jonboulle/clockwork"
+	"github.com/sirupsen/logrus"
 	"go.opentelemetry.io/otel/attribute"
 	oteltrace "go.opentelemetry.io/otel/trace"
 	"golang.org/x/crypto/ssh"
@@ -2041,6 +2042,10 @@ func serializeVersion(format string, proxyVersion string, proxyPublicAddress str
 
 // onLogin logs in with remote proxy and gets signed certificates
 func onLogin(cf *CLIConf, reExecArgs ...string) error {
+	logrus.WithFields(logrus.Fields{
+		"proxy":    cf.Proxy,
+		"username": cf.Username,
+	}).Debug("üêõ [DEBUG] inside onLogin() with CLIConf")
 	autoRequest := true
 	// special case: --request-roles=no disables auto-request behavior.
 	if cf.DesiredRoles == "no" {
